{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad05698e-dc7a-4246-9a44-e3728696baa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: mxnet\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import os\n",
    "\n",
    "os.environ['DGLBACKEND'] = 'mxnet'\n",
    "\n",
    "import math\n",
    "from mxnet import gluon, nd\n",
    "from utils import collate, rmse_trainset, rmse_testset, GraphTraffic, \\\n",
    "    load_adjmatrix, EarlyStopping, train_seed, StandardScaler, masked_rmse_np, masked_mae_np, masked_mape_np\n",
    "from model import SARGCN\n",
    "import argparse\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0e38ad7-f833-4339-9280-797e13be955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def step(self, acc, model):\n",
    "        score = acc\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(model)\n",
    "        elif score > self.best_score:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(model)\n",
    "            self.counter = 0\n",
    "        return self.early_stop\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        model.save_parameters('SARGCN.param')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d3542b8-5d26-48b3-929f-2b2959f8f3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "train_seed(seed=12345)\n",
    "device = mx.gpu(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a151c73-ff71-4f46-9bfa-58b088e6ac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # load data\n",
    "    g, tratim_matrix = load_adjmatrix('../data/adjacency_matrix_13.mat')\n",
    "    edge_type = np.load('../data/edge_type_13.npy')\n",
    "    edge_type = mx.nd.array(edge_type, ctx=device)\n",
    "\n",
    "    # load data\n",
    "    with open('../data/train.pkl', 'rb') as f:\n",
    "        train = pickle.load(f)\n",
    "\n",
    "    with open('../data/val.pkl', 'rb') as f:\n",
    "        val = pickle.load(f)\n",
    "\n",
    "    with open('../data/test.pkl', 'rb') as f:\n",
    "        test = pickle.load(f)\n",
    "\n",
    "    train_x, train_y = train['x'], train['y']\n",
    "    valid_x, valid_y = val['x'], val['y']\n",
    "    test_x, test_y = test['x'], test['y']\n",
    "\n",
    "    train_x = np.swapaxes(train_x, 1, 2)\n",
    "    train_x = np.concatenate([train_x[:, :, :, 0], train_x[:, :, :, 1]], axis=2)\n",
    "    train_y = np.swapaxes(train_y, 1, 2)\n",
    "    train_y = np.concatenate([train_y[:, :, :, 0], train_y[:, :, :, 1]], axis=2)\n",
    "\n",
    "    valid_x = np.swapaxes(valid_x, 1, 2)\n",
    "    valid_x = np.concatenate([valid_x[:, :, :, 0], valid_x[:, :, :, 1]], axis=2)\n",
    "    valid_y = np.swapaxes(valid_y, 1, 2)\n",
    "    valid_y = np.concatenate([valid_y[:, :, :, 0], valid_y[:, :, :, 1]], axis=2)\n",
    "\n",
    "    test_x = np.swapaxes(test_x, 1, 2)\n",
    "    test_x = np.concatenate([test_x[:, :, :, 0], test_x[:, :, :, 1]], axis=2)\n",
    "    test_y = np.swapaxes(test_y, 1, 2)\n",
    "    test_y = np.concatenate([test_y[:, :, :, 0], test_y[:, :, :, 1]], axis=2)\n",
    "\n",
    "    scaler_axis = (0, 1, 2)\n",
    "    scaler = StandardScaler(mean=train_x.mean(axis=scaler_axis),\n",
    "                            std=train_x.std(axis=scaler_axis))\n",
    "    train_x = nd.array(train_x, ctx=device)\n",
    "    train_y = nd.array(train_y, ctx=device)\n",
    "    valid_x = nd.array(valid_x, ctx=device)\n",
    "    valid_y = nd.array(valid_y, ctx=device)\n",
    "    test_x = nd.array(test_x, ctx=device)\n",
    "    test_y = nd.array(test_y, ctx=device)\n",
    "\n",
    "    train_x = scaler.transform(train_x)\n",
    "    train_y = scaler.transform(train_y)\n",
    "    valid_x = scaler.transform(valid_x)\n",
    "    valid_y = scaler.transform(valid_y)\n",
    "    test_x = scaler.transform(test_x)\n",
    "    test_y = scaler.transform(test_y)\n",
    "\n",
    "    num_train = train_x.shape[0]\n",
    "    num_valid = valid_x.shape[0]\n",
    "    num_test = test_x.shape[0]\n",
    "\n",
    "    seq_len = 4\n",
    "    pre_len = 4\n",
    "    num_node = 80\n",
    "\n",
    "    # divide training set, valid set and test set\n",
    "    trainset = GraphTraffic(num_train, num_node, tratim_matrix)\n",
    "    validset = GraphTraffic(num_valid, num_node, tratim_matrix)\n",
    "    testset = GraphTraffic(num_test, num_node, tratim_matrix)\n",
    "\n",
    "    for i in range(trainset.__len__()):\n",
    "        trainset.graphs[i].ndata['h'] = train_x[i]\n",
    "        trainset.graphs[i].edata['type'] = edge_type.reshape(-1, )\n",
    "        trainset.labels[i] = train_y[i]\n",
    "    for i in range(validset.__len__()):\n",
    "        validset.graphs[i].ndata['h'] = valid_x[i]\n",
    "        validset.graphs[i].edata['type'] = edge_type.reshape(-1, )\n",
    "        validset.labels[i] = valid_y[i]\n",
    "    for i in range(testset.__len__()):\n",
    "        testset.graphs[i].ndata['h'] = test_x[i]\n",
    "        testset.graphs[i].edata['type'] = edge_type.reshape(-1, )\n",
    "        testset.labels[i] = test_y[i]\n",
    "\n",
    "    return scaler, trainset, validset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c9db343-d356-49ea-bc89-75b1bf0e4514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    scaler, trainset, validset, testset = load_data()\n",
    "    dropout = 0.05\n",
    "    in_feats = 2*4\n",
    "    num_classes = 2*4\n",
    "    lr = 1e-3\n",
    "    weight_decay = 0.0\n",
    "    n_bases = 1\n",
    "    n_layers = 1\n",
    "    n_epochs = 1\n",
    "    num_rels = 25\n",
    "    use_self_loop = True\n",
    "    batch_size = 32\n",
    "    test_batch_size = 32\n",
    "    test_batch_size = 32\n",
    "    n_hidden = 224\n",
    "    n_channel = 96\n",
    "    seq_len = 4\n",
    "    num_nodes = 80\n",
    "\n",
    "    # create model\n",
    "    model = SARGCN(in_feats,\n",
    "                   n_hidden,\n",
    "                   num_classes,\n",
    "                   num_rels,\n",
    "                   inter_channel=n_channel,\n",
    "                   num_bases=n_bases,\n",
    "                   num_hidden_layers=n_layers,\n",
    "                   dropout=dropout,\n",
    "                   use_self_loop=use_self_loop,\n",
    "                   gpu_id=mx.gpu(),\n",
    "                   residual=False)\n",
    "    model.initialize(mx.init.Xavier(magnitude=math.sqrt(2.0)), ctx=mx.gpu())\n",
    "    trainer = gluon.Trainer(model.collect_params(),\n",
    "                            'adam',\n",
    "                            {'learning_rate': lr,\n",
    "                             'wd': weight_decay})\n",
    "    loss = gluon.loss.L2Loss()\n",
    "    stopper = EarlyStopping(patience=15)\n",
    "    train_iter = gluon.data.DataLoader(trainset, batch_size, shuffle=False,\n",
    "                                       batchify_fn=collate, last_batch='discard',\n",
    "                                       thread_pool=True)\n",
    "    valid_iter = gluon.data.DataLoader(validset, test_batch_size, shuffle=False,\n",
    "                                       batchify_fn=collate, last_batch='discard',\n",
    "                                       thread_pool=True)\n",
    "    test_iter = gluon.data.DataLoader(testset, test_batch_size, shuffle=False,\n",
    "                                      batchify_fn=collate, last_batch='discard',\n",
    "                                      thread_pool=True)\n",
    "    TrainLoss, ValidLoss, TestLoss = [], [], []\n",
    "\n",
    "    for epoch in range(1):\n",
    "        train_l_sum, n = 0.0, 0\n",
    "        start_time = time.time()\n",
    "        for iter, (bg, label) in enumerate(train_iter):\n",
    "            model.g = bg\n",
    "            with mx.autograd.record():\n",
    "                edge_type = bg.edata['type']\n",
    "                pred = model(bg, bg.ndata['h'], edge_type, None)\n",
    "                l = loss(pred, label)\n",
    "            break\n",
    "        break\n",
    "\n",
    "    # evaluate\n",
    "    model_list = ['pre_trained_model.param']\n",
    "\n",
    "    for filename in model_list:\n",
    "        print(filename)\n",
    "        model.load_parameters(filename)\n",
    "        test_rmse, test_mse, test_mae, test_mape, true, pred = rmse_testset(model, test_iter, batch_size, num_nodes, scaler, mae=True)\n",
    "        for i in range(seq_len):\n",
    "            rmse = masked_rmse_np(labels=true[:, :, [i, i+4]], preds=pred[:, :, [i, i+4]])\n",
    "            mae = masked_mae_np(labels=true[:, :, [i, i+4]], preds=pred[:, :, [i, i+4]])\n",
    "            mape = masked_mape_np(labels=true[:, :, [i, i+4]], preds=pred[:, :, [i, i+4]])\n",
    "            print('time: %i, rmse: %.3f, mae: %.3f, mape: %.5f' % ((i+1)*15, rmse, mae, mape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df3c430f-d5a4-4455-a9ca-d7b093390284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of nodes: 80\n",
      "number of edges: 715\n",
      "pre_trained_model.param\n",
      "time: 15, rmse: 36.220, mae: 22.477, mape: 0.13942\n",
      "time: 30, rmse: 37.825, mae: 23.464, mape: 0.14986\n",
      "time: 45, rmse: 39.408, mae: 24.431, mape: 0.16254\n",
      "time: 60, rmse: 41.589, mae: 25.294, mape: 0.17597\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4dc54e-2212-4d48-93dc-11f57ce09442",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jie_zeng",
   "language": "python",
   "name": "jie_zeng"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
